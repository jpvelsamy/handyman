package in.handyman.raven.lib;

import com.zaxxer.hikari.HikariDataSource;
import in.handyman.raven.exception.HandymanException;
import in.handyman.raven.lambda.access.ResourceAccess;
import in.handyman.raven.lambda.action.ActionExecution;
import in.handyman.raven.lambda.action.IActionExecution;
import in.handyman.raven.lambda.doa.audit.ActionExecutionAudit;
import in.handyman.raven.lambda.process.HRequestResolver;
import in.handyman.raven.lambda.process.LContext;
import in.handyman.raven.lib.model.ForkProcess;
import in.handyman.raven.util.CommonQueryUtil;
import org.slf4j.Logger;
import org.slf4j.Marker;
import org.slf4j.MarkerFactory;

import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

/**
 * Auto Generated By Raven
 */
@ActionExecution(actionName = "ForkProcess")
public class ForkProcessAction implements IActionExecution {

    private final ActionExecutionAudit actionExecutionAudit;
    private final Logger log;
    private final ForkProcess forkProcess;

    private final Marker aMarker;

    public ForkProcessAction(final ActionExecutionAudit actionExecutionAudit, final Logger log, final Object forkProcess) {
        this.forkProcess = (ForkProcess) forkProcess;
        this.actionExecutionAudit = actionExecutionAudit;
        this.log = log;
        this.aMarker = MarkerFactory.getMarker("ForkProcess");
    }

    @Override
    public void execute() throws Exception {
        var name = forkProcess.getName();
        var fileRelativePath = forkProcess.getSource();
        var processName = forkProcess.getTarget();
        var dbSrc = forkProcess.getDatasource();
        var forkBatchSize = forkProcess.getForkBatchSize();
        var forkBatchSizeInt = (forkBatchSize != null && !forkBatchSize.isEmpty()) ? Integer.parseInt(forkBatchSize) : 1;
        var sql = forkProcess.getValue();
        var pipelineId = actionExecutionAudit.getPipelineId();

        log.info(aMarker, " id#{}, name#{}, forked process#{}, calledFile#{}, db=#{}", pipelineId, name, processName, fileRelativePath, dbSrc);
        final HikariDataSource source = ResourceAccess.rdbmsConn(dbSrc);
        final Set<LContext> lContexts = new HashSet<>();
        try (var conn = source.getConnection()) {
            try (var stmt = conn.createStatement()) {
                final Map<String, String> context = actionExecutionAudit.getContext();
                try (var rs = stmt.executeQuery(sql)) {
                    var columnCount = rs.getMetaData().getColumnCount();
                    while (rs.next()) {
                        CommonQueryUtil.addKeyConfig(context, log,
                                rs, columnCount, "");
                        final LContext lContext = LContext.builder()
                                .inheritedContext(context)
                                .lambdaName(actionExecutionAudit.getLambdaName())
                                .parentActionId(actionExecutionAudit.getActionId())
                                .parentActionName(actionExecutionAudit.getActionName())
                                .relativePath(fileRelativePath)
                                .processLoadType(HRequestResolver.LoadType.FILE.name())
                                .pipelineName(processName)
                                .parentPipelineId(actionExecutionAudit.getPipelineId())
                                .parentPipelineName(actionExecutionAudit.getPipelineName())
                                .rootPipelineId(actionExecutionAudit.getRootPipelineId())
                                .build();
                        lContexts.add(lContext);
                    }
                }
            }
        }
        var executor = Executors.newWorkStealingPool();
        final AtomicInteger counter = new AtomicInteger();
        final AtomicInteger batchNoAtomic = new AtomicInteger(1);
        lContexts.stream()
                .collect(Collectors.groupingBy(it -> counter.getAndIncrement() / forkBatchSizeInt))
                .values().forEach(lContextSublist -> {
                    final int batchNo = batchNoAtomic.getAndIncrement();
                    final int size = lContextSublist.size();
                    log.info(aMarker, "Started Executing Fork-Process for batch-id#{} with batch-size#{}", batchNo, size);
                    var countDownLatch = new CountDownLatch(size);
                    lContextSublist.forEach(lContext -> {
                        var processWorker = new LambdaCallable(lContext, countDownLatch);
                        executor.submit(processWorker);
                    });
                    try {
                        countDownLatch.await();
                    } catch (InterruptedException e) {
                        log.error(aMarker, "ForkProcess failed for batch " + batchNo, e);
                        throw new HandymanException("ForkProcess failed for batch " + batchNo, e);
                    }
                });


    }

    @Override
    public boolean executeIf() throws Exception {
        return forkProcess.getCondition();
    }
}
